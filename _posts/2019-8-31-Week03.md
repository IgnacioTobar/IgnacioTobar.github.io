---
layout: post
title: Week 03
---

## Performance of recommender algorithms on top-n recommendation tasks

In many applications, the objective of the recommender system is to find a few specific items which are supposed to be most appealing to the user (topN). Common methodologies based on error metrics (such as RMSE) are not a natural fit for evaluating the topN recommendation task. Which is why the paper focus on evaluating alternative methodologies based on accuracy metrics (precision/recall). 
 

The paper shows that there's no direct relation between error and accuracy metrics, and that the latter usually serves as a better evaluation of the algorithm. It also proposes a simple algorithm geared towards improving the accuracy instead of the error, obtaining better results than other more sophisticated algorithms focused on RSME.  
This is important as it can indicate a need for a reevaluation of the goals of the current algorithms and may serve as inspiration for new and different methods.


If there's something to critique about the paper, is that the graphs where it shows the result of the tests are kind of hard to read, different color for each line would make it easier to read.



## Evaluating recommendation systems

The paper extensively describes the different ways to evaluate the properties of recommender systems. It first starts explaining the three usual way to test these systems: Offline Experiments (the kind of experiments we usually do in class), User Studies: tests done with a set of test subjects to collect quantitative and qualitative feedback and Online Evaluation: implementing the recommender system in a real site to be used by real users to measure the effect it has.


It also explains the different properties of these systems: Accuracy, Coverage, Trust, Novelty, Serendipity, Diversity, etc. While it extensively explain each one, only a few properties have explicit metrics or ways to calculate them described in the paper. Also, they are mostly explained independently from each other, while testing a recommender system you would like to take into account as many properties as possible, but the paper fails to properly link them together, nor explain when you would prefer to use one over the other.


Another problem I found, is that while at the start it talks about the different ways to test these systems (offline, user tests, online), much like the different properties, it doesn't expand on it nor try the integrate, either between them, or with the other properties.



## Evaluating collaborative filtering recommender systems

Like the previous paper, this one focus on discussing the different ways to evaluate recommender systems.


One thing that differentiates this paper from the other one, is that it goes in deep not only on the different properties to evaluate, but also the user *tasks* for which recommender systems are used. This is quite logical, as it is necessary to know the intended objective of the recommender system, so that it is possible to evaluate if the system is working as intended or not. Some of these tasks are: 

* Find Good Items
* Recommend Sequence
* Find Credible Recommender


On the discussion of the properties to evaluate, compared to the other paper, this one is much more technical, offering formulas or in deep explanation on how to implement the metrics to evaluate. In a way this paper and the previous one complement each other nicely, together providing a complete explanation of each property.




